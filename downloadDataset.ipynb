{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MattingMen Proudly Present: Image Matting, Neural and Classical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import pypardiso\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "##ðŸš¨ðŸš¨ðŸš¨ UNCOMMENT TO DOWNLOAD 15gig DATASET ðŸš¨ðŸš¨ðŸš¨\n",
    "#Dataset used: https://www.kaggle.com/datasets/laurentmih/aisegmentcom-matting-human-datasets?select=matting\n",
    "#\"30gigs\" on kaggle is a lie, dataset is internally duplicated\n",
    "\n",
    "# def download_and_extract(zip_url, extract_to='./archive/', zip_path='temp.zip'):\n",
    "#     file_id = zip_url.split('/')[-2]\n",
    "#     drive_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "#     gdown.download(drive_url, zip_path, quiet=False)\n",
    "    \n",
    "#     with ZipFile(zip_path, 'r') as zip_ref: \n",
    "#         zip_ref.extractall(extract_to)\n",
    "#     os.remove(zip_path)\n",
    "\n",
    "# google_drive_link = \"https://drive.google.com/file/d/1ZWjgJ762I40bYn9DC9IDAsDbvwJ8Dqm5/view?usp=drive_link\"\n",
    "# download_and_extract(google_drive_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_dataframe(root_dir):\n",
    "    data = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if 'clip_' in subdir:\n",
    "                image_type = 'clip'\n",
    "            elif 'matting_' in subdir:\n",
    "                image_type = 'matting'\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            filename_without_extension = os.path.splitext(file)[0]\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            \n",
    "            data.append([filename_without_extension, full_path, image_type])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['image_name', 'path', 'type'])\n",
    "    return df\n",
    "\n",
    "root_directory = './archive/'\n",
    "image_df = create_image_dataframe(root_directory)\n",
    "\n",
    "print(image_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Closed Form Matting Implementation \n",
    "https://github.com/MarcoForte/closed-form-matting\n",
    "\n",
    "Full credit to the original paper/ authors. \n",
    "We DID go through the effort of understanding their code and loading it accordingly. We also replaced their matrix solver with a multithreaded variant using intel's mkl backend, significantly speeding up the closed form implementation computations. The code below represents strictly the alpha matting implementation and has been carefully severed from the author's other work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This module implements natural image matting method described in:\n",
    "    Levin, Anat, Dani Lischinski, and Yair Weiss. \"A closed-form solution to natural image matting.\"\n",
    "    IEEE Transactions on Pattern Analysis and Machine Intelligence 30.2 (2008): 228-242.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def _rolling_block(A, block=(3, 3)):\n",
    "    \"\"\"Applies sliding window to given matrix.\"\"\"\n",
    "    shape = (A.shape[0] - block[0] + 1, A.shape[1] - block[1] + 1) + block\n",
    "    strides = (A.strides[0], A.strides[1]) + A.strides\n",
    "    return as_strided(A, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def compute_laplacian(img: np.ndarray, mask=None, eps: float =10**(-7), win_rad: int =1):\n",
    "    \"\"\"Computes Matting Laplacian for a given image.\n",
    "\n",
    "    Args:\n",
    "        img: 3-dim numpy matrix with input image\n",
    "        mask: mask of pixels for which Laplacian will be computed.\n",
    "            If not set Laplacian will be computed for all pixels.\n",
    "        eps: regularization parameter controlling alpha smoothness\n",
    "            from Eq. 12 of the original paper. Defaults to 1e-7.\n",
    "        win_rad: radius of window used to build Matting Laplacian (i.e.\n",
    "            radius of omega_k in Eq. 12).\n",
    "    Returns: sparse matrix holding Matting Laplacian.\n",
    "    \"\"\"\n",
    "\n",
    "    win_size = (win_rad * 2 + 1) ** 2\n",
    "    h, w, d = img.shape\n",
    "    # Number of window centre indices in h, w axes\n",
    "    c_h, c_w = h - 2 * win_rad, w - 2 * win_rad\n",
    "    win_diam = win_rad * 2 + 1\n",
    "\n",
    "    indsM = np.arange(h * w).reshape((h, w))\n",
    "    ravelImg = img.reshape(h * w, d)\n",
    "    win_inds = _rolling_block(indsM, block=(win_diam, win_diam))\n",
    "\n",
    "    win_inds = win_inds.reshape(c_h, c_w, win_size)\n",
    "    if mask is not None:\n",
    "        mask = cv2.dilate(\n",
    "            mask.astype(np.uint8),\n",
    "            np.ones((win_diam, win_diam), np.uint8)\n",
    "        ).astype(bool)\n",
    "        win_mask = np.sum(mask.ravel()[win_inds], axis=2)\n",
    "        win_inds = win_inds[win_mask > 0, :]\n",
    "    else:\n",
    "        win_inds = win_inds.reshape(-1, win_size)\n",
    "\n",
    "    \n",
    "    winI = ravelImg[win_inds]\n",
    "\n",
    "    win_mu = np.mean(winI, axis=1, keepdims=True)\n",
    "    win_var = np.einsum('...ji,...jk ->...ik', winI, winI) / win_size - np.einsum('...ji,...jk ->...ik', win_mu, win_mu)\n",
    "\n",
    "    A = win_var + (eps/win_size)*np.eye(3)\n",
    "    B = (winI - win_mu).transpose(0, 2, 1)\n",
    "    X = np.linalg.solve(A, B).transpose(0, 2, 1)\n",
    "    vals = np.eye(win_size) - (1.0/win_size)*(1 + X @ B)\n",
    "\n",
    "    nz_indsCol = np.tile(win_inds, win_size).ravel()\n",
    "    nz_indsRow = np.repeat(win_inds, win_size).ravel()\n",
    "    nz_indsVal = vals.ravel()\n",
    "\n",
    "    # rewrite L in CSR format\n",
    "    L = scipy.sparse.coo_matrix((nz_indsVal, (nz_indsRow, nz_indsCol)), shape=(h*w, h*w))\n",
    "    return L\n",
    "\n",
    "\n",
    "def closed_form_matting_with_prior(image, prior, prior_confidence, consts_map=None):\n",
    "    \"\"\"Applies closed form matting with prior alpha map to image.\n",
    "\n",
    "    Args:\n",
    "        image: 3-dim numpy matrix with input image.\n",
    "        prior: matrix of same width and height as input image holding apriori alpha map.\n",
    "        prior_confidence: matrix of the same shape as prior hodling confidence of prior alpha.\n",
    "        consts_map: binary mask of pixels that aren't expected to change due to high\n",
    "            prior confidence.\n",
    "\n",
    "    Returns: 2-dim matrix holding computed alpha map.\n",
    "    \"\"\"\n",
    "\n",
    "    assert image.shape[:2] == prior.shape, ('prior must be 2D matrix with height and width equal '\n",
    "                                            'to image.')\n",
    "    assert image.shape[:2] == prior_confidence.shape, ('prior_confidence must be 2D matrix with '\n",
    "                                                       'height and width equal to image.')\n",
    "    assert (consts_map is None) or image.shape[:2] == consts_map.shape, (\n",
    "        'consts_map must be 2D matrix with height and width equal to image.')\n",
    "\n",
    "    print('Computing Matting Laplacian.')\n",
    "    laplacian = compute_laplacian(image, ~consts_map if consts_map is not None else None)\n",
    "    confidence = scipy.sparse.diags(prior_confidence.ravel())\n",
    "    print('Solving for alpha.')\n",
    "    solution = pypardiso.spsolve(\n",
    "        laplacian + confidence,\n",
    "        prior.ravel() * prior_confidence.ravel()\n",
    "    )\n",
    "    alpha = np.minimum(np.maximum(solution.reshape(prior.shape), 0), 1)\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def closed_form_matting_with_scribbles(image, scribbles, scribbles_confidence=100.0):\n",
    "    \"\"\"Apply Closed-Form matting to given image using scribbles image.\"\"\"\n",
    "\n",
    "    assert image.shape[:2] == scribbles.shape, 'scribbles must have exactly same shape as image.'\n",
    "    consts_map = scribbles != 0.5\n",
    "    return closed_form_matting_with_prior(\n",
    "        image,\n",
    "        scribbles,\n",
    "        scribbles_confidence * consts_map,\n",
    "        consts_map\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below squiggle generator performs image erosion so as to deterministically place \"squiggles\" at points resembling the centers of the foreground / background regions, as a human might.\n",
    "def get_squiggles(image, colorimage, threshold=4, area=0.1):\n",
    "    kernel = np.array([\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 0]\n",
    "    ], dtype=np.uint8)\n",
    "    \n",
    "    #set up the foreground and background squiggles\n",
    "    target_alpha = image.copy()\n",
    "    background_alpha = 255-image.copy()\n",
    "    \n",
    "    #get mask ratios set up\n",
    "    mask1_initial_total = np.sum(image)\n",
    "    mask2_initial_total = np.sum(255-image)\n",
    "    mask1_area = mask1_initial_total\n",
    "    mask2_area = mask2_initial_total\n",
    "\n",
    "    #erode both until they're 10% of the original area (proudly our own idea :D)\n",
    "    while(mask1_area / mask1_initial_total > area):\n",
    "        neighbor_count = cv2.filter2D((target_alpha > 0).astype(np.uint8), -1, kernel, borderType=cv2.BORDER_CONSTANT)\n",
    "        target_alpha[(neighbor_count < threshold) & (target_alpha > 0)] = 0\n",
    "        mask1_area = np.sum(target_alpha)\n",
    "\n",
    "    while(mask2_area / mask2_initial_total > area):\n",
    "        neighbor_count = cv2.filter2D((background_alpha >0).astype(np.uint8), -1, kernel, borderType=cv2.BORDER_CONSTANT)\n",
    "        background_alpha[(neighbor_count < threshold) & (background_alpha > 0)] = 0\n",
    "        mask2_area = np.sum(background_alpha)\n",
    "\n",
    "    #some arbitrary blurring to help tackle regions that shrank akwardly / too uniformly\n",
    "    target_alpha=cv2.blur(target_alpha, (20,20)) > 1 \n",
    "    background_alpha=cv2.blur(background_alpha,(20,20)) > 1\n",
    "\n",
    "\n",
    "    #at this point, we explored several alternative ideas, that each usually began with k-means\n",
    "    #as it turns out, our closed form solution is hoping that we not only place scribbles appropriately on the foreground and background\n",
    "    #but that we also capture some of the colors in our squiggle region that most closely represent this foreground or background color composition\n",
    "\n",
    "    #unfortunately, our squiggle erosion algorithm is naive to color. The thought was then: \n",
    "    #the \"select top N\" colors idea was our own, though googling helped us decide between clustering options. \"Eigencolors\" lead to an interesting side quest learning about eigengrau\n",
    "    \n",
    "    # 1.) let's select some colors using K means from within the squiggle region\n",
    "    # 2.) let's find the closest pixels to these colors in the squiggle region\n",
    "    # 3.) let's drop circle squiggles on these points and use that instead.\n",
    "    \n",
    "    # Reshape the image to be a list of pixels\n",
    "\n",
    "    pixels = colorimage.reshape(-1, 3)\n",
    "    foreground_mask = target_alpha.reshape(-1,3)\n",
    "    background_mask = background_alpha.reshape(-1,3)\n",
    "\n",
    "    foreground_squiggle_pixels = pixels[foreground_mask == 1]\n",
    "    background_squiggle_pixels = pixels[background_mask == 1]\n",
    "    k = 3  # Number of clusters\n",
    "    clf = KMeans(n_clusters=k)\n",
    "    foreground_labels = clf.fit_predict(foreground_squiggle_pixels)\n",
    "    foreground_palette = clf.cluster_centers_\n",
    "    display(foreground_palette)\n",
    "\n",
    "    clf = KMeans(n_clusters=k)\n",
    "    background_labels = clf.fit_predict(background_squiggle_pixels)\n",
    "    background_palette = clf.cluster_centers_\n",
    "    display(background_palette)\n",
    "    \n",
    "    canvas = np.ones_like(image)\n",
    "    #nearest neighbors pixelwise by euclidean color dist\n",
    "    for color in foreground_palette:\n",
    "        color_pixels_in_mask = colorimage[target_alpha == 1]\n",
    "        mask_indices = np.argwhere(target_alpha == 1)\n",
    "        distances = np.linalg.norm(color_pixels_in_mask - target_color, axis=1)\n",
    "        closest_index = np.argmin(distances)\n",
    "        closest_point = mask_indices[closest_index]\n",
    "        cv2.circle(canvas, (closest_point[1], closest_point[0]), 10, (255, 255, 255), -1)\n",
    "        \n",
    "\n",
    "\n",
    "    #throw the squiggles together on a canvas of grey as the closed form implementation seems to expect\n",
    "    canvas = np.ones_like(image)\n",
    "    canvas = canvas/2. + target_alpha/2. - background_alpha/2.\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_pairs(df, num_images, model):\n",
    "    fontsize = 10\n",
    "    \n",
    "    matting_images = df[df['type'] == 'matting']\n",
    "    clip_images = df[df['type'] == 'clip']\n",
    "    \n",
    "    selected_matting = matting_images.sample(n=num_images)\n",
    "    \n",
    "    _, axs = plt.subplots(5, num_images, figsize=(40, 15))\n",
    "    \n",
    "    for i, (_, row) in enumerate(selected_matting.iterrows()):\n",
    "        matting_img = cv2.imread(row['path'], cv2.IMREAD_UNCHANGED)\n",
    "        imgname = row['image_name'].split(\"-\")[1]\n",
    "        clip_row = clip_images[clip_images['image_name'] == row['image_name']].iloc[0]\n",
    "        scribbles = get_squiggles(matting_img[:,:,3], threshold=4, area=.005)\n",
    "        clip_img = cv2.imread(str(clip_row['path']))\n",
    "        clip_img = cv2.cvtColor(clip_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #show ground truth\n",
    "        axs[0, i].imshow(matting_img[:,:,3], cmap='gray')\n",
    "        axs[0, i].set_title(f\"Matting: {imgname}\", size=fontsize)\n",
    "        axs[0, i].axis('off')\n",
    "        \n",
    "        #show color\n",
    "        axs[1, i].imshow(clip_img)\n",
    "        axs[1, i].set_title(f\"Clip: {imgname}\", size=fontsize)\n",
    "        axs[1, i].axis('off')\n",
    "\n",
    "        #Show scribbles\n",
    "        axs[2, i].imshow(scribbles)\n",
    "        axs[2, i].set_title(f\"Squiggles: {imgname}\", size=fontsize)\n",
    "        axs[2, i].axis('off')\n",
    "\n",
    "        #Show alpha\n",
    "        scaled_image = cv2.resize(clip_img, (512, 512))\n",
    "        scribbles_resized = cv2.resize(scribbles, (512,512))\n",
    "        alpha = closed_form_matting_with_scribbles(scaled_image/255.0, scribbles_resized)\n",
    "        alpha = cv2.resize(alpha, (600,800))\n",
    "        axs[3, i].imshow(alpha, cmap='gray')\n",
    "        axs[3, i].set_title(f\"C.F. Matte: {imgname}\", size=fontsize)\n",
    "        axs[3, i].axis('off')\n",
    "\n",
    "        #show model output\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            model_input = transform((scaled_image/255.).astype(np.float32)).unsqueeze(0)\n",
    "            model_input = model_input.to('cuda')\n",
    "            print(model_input.shape)\n",
    "            out = np.array(np.transpose(model(model_input)[0].cpu(),(1,2,0)))\n",
    "        out = cv2.resize(out, (600,800))\n",
    "        axs[4, i].imshow(out, cmap='gray')\n",
    "        axs[4, i].set_title(f\"Unet Matte: {imgname}\", size=fontsize)\n",
    "        axs[4, i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "model = torch.load(\"./best_model.pt\")\n",
    "display_image_pairs(image_df, 16, model)\n",
    "\n",
    "def load_img(df, idx, normalize_to_float=False):\n",
    "    matting_row = df[df['type'] == 'matting'].iloc[idx]\n",
    "    clip_row = df[df['type'] == 'clip'].iloc[idx]\n",
    "    matting_img = cv2.imread(matting_row['path'], cv2.IMREAD_UNCHANGED)[:,:,3]\n",
    "    clip_img = cv2.imread(str(clip_row['path']))\n",
    "    clip_img = cv2.cvtColor(clip_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if(normalize_to_float):\n",
    "        clip_img = clip_img / 255.0\n",
    "        matting_img = matting_img / 255.0\n",
    "    return matting_img.astype(np.float32), clip_img.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Time!\n",
    "Below we train a U-Net on a fraction of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from torch.nn import Module, Conv2d, MaxPool2d, ReLU, BCEWithLogitsLoss, ConvTranspose2d, BatchNorm2d\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Credit to https://medium.com/analytics-vidhya/unet-implementation-in-pytorch-idiot-developer-da40d955f201\n",
    "\n",
    "class conv_block(Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = BatchNorm2d(out_c)\n",
    "        self.conv2 = Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = BatchNorm2d(out_c)\n",
    "        self.relu = ReLU()\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class encoder_block(Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.up = ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "    \n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "#Set up the unet\n",
    "#Originally attempted hand implementation but faced substantial challenges. \n",
    "#Credit again to medium article linked above for this one\n",
    "class Unet(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        self.outputs = Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s1, p1 = self.e1(x)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        b = self.b(p4)\n",
    "\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        outputs = self.outputs(d4)\n",
    "\n",
    "        return outputs\n",
    "        \n",
    "transform = transforms.Compose([transforms.ToTensor()]) #better practice might be to integrate resizing here\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df: DataFrame, n=20):\n",
    "        self.images = df[df['type'] == 'matting']['image_name']\n",
    "        self.images = self.images.sample(n, random_state=123)\n",
    "        self.df = df[df['image_name'].isin(self.images)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        matting, color = load_img(self.df, idx, True)\n",
    "        \n",
    "        matting = cv2.resize(matting, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "        color = cv2.resize(color, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        color = transform(color)\n",
    "        matting = transform(matting)\n",
    "        return color, matting\n",
    "    \n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, df: DataFrame, n=20):\n",
    "        self.images = df[df['type'] == 'matting']['image_name']\n",
    "        self.images = self.images.sample(n, random_state=321)\n",
    "        self.df = df[df['image_name'].isin(self.images)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        matting, color = load_img(self.df, idx, True)\n",
    "        \n",
    "        matting = cv2.resize(matting, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "        color = cv2.resize(color, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        color = transform(color)\n",
    "        matting = transform(matting)\n",
    "        return color, matting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 Train Loss: 0.589528754234314 Test Loss: 0.8159571627775828\n",
      "Epoch1 Train Loss: 0.5128450975418091 Test Loss: 0.4531466841697693\n",
      "Epoch2 Train Loss: 0.443076553106308 Test Loss: 0.45786391496658324\n",
      "Epoch3 Train Loss: 0.4234321517944336 Test Loss: 0.4112814704577128\n",
      "Epoch4 Train Loss: 0.40623291659355165 Test Loss: 0.4299197256565094\n",
      "Epoch5 Train Loss: 0.3801400821208954 Test Loss: 0.3878709703683853\n",
      "Epoch6 Train Loss: 0.37262420427799225 Test Loss: 0.3364482243855794\n",
      "Epoch7 Train Loss: 0.3482739003896713 Test Loss: 0.3153642912705739\n",
      "Epoch8 Train Loss: 0.3274239305257797 Test Loss: 0.31631338198979697\n",
      "Epoch9 Train Loss: 0.3110790729522705 Test Loss: 0.3237533926963806\n",
      "Epoch10 Train Loss: 0.29819971418380736 Test Loss: 0.27348031202952067\n",
      "Epoch11 Train Loss: 0.28851374340057373 Test Loss: 0.3141932666301727\n",
      "Epoch12 Train Loss: 0.2716829208135605 Test Loss: 0.33446110288302106\n",
      "Epoch13 Train Loss: 0.2576158444881439 Test Loss: 0.25431721011797587\n",
      "Epoch14 Train Loss: 0.24724108970165254 Test Loss: 0.23895085453987122\n",
      "Epoch15 Train Loss: 0.2371253843307495 Test Loss: 0.24454720119635265\n",
      "Epoch16 Train Loss: 0.22053261017799378 Test Loss: 0.2351548567414284\n",
      "Epoch17 Train Loss: 0.21321089154481887 Test Loss: 0.25249693393707273\n",
      "Epoch18 Train Loss: 0.209271404504776 Test Loss: 0.21999732553958892\n",
      "Epoch19 Train Loss: 0.20147937268018723 Test Loss: 0.2232895056406657\n",
      "Epoch20 Train Loss: 0.18889589339494706 Test Loss: 0.2316639930009842\n",
      "Epoch21 Train Loss: 0.17827716863155366 Test Loss: 0.1960413510600726\n",
      "Epoch22 Train Loss: 0.1706772786974907 Test Loss: 0.2233238453666369\n",
      "Epoch23 Train Loss: 0.163024100959301 Test Loss: 0.1894020050764084\n",
      "Epoch24 Train Loss: 0.15834911251068115 Test Loss: 0.18611862063407897\n",
      "Epoch25 Train Loss: 0.15457848969101906 Test Loss: 0.18614557683467864\n",
      "Epoch26 Train Loss: 0.14970344209671022 Test Loss: 0.17668908188740412\n",
      "Epoch27 Train Loss: 0.1356568029820919 Test Loss: 0.18255405922730764\n",
      "Epoch28 Train Loss: 0.13401636800169944 Test Loss: 0.20443750073512396\n",
      "Epoch29 Train Loss: 0.12845052742958069 Test Loss: 0.16420706907908122\n",
      "Epoch30 Train Loss: 0.12438521483540535 Test Loss: 0.21902682781219482\n",
      "Epoch31 Train Loss: 0.12337612879276276 Test Loss: 0.1533448671301206\n",
      "Epoch32 Train Loss: 0.11499953335523605 Test Loss: 0.14936818927526474\n",
      "Epoch33 Train Loss: 0.10608736798167229 Test Loss: 0.1669240320722262\n",
      "Epoch34 Train Loss: 0.1027801234126091 Test Loss: 0.15026215687394143\n",
      "Epoch35 Train Loss: 0.08946846827864646 Test Loss: 0.16508252968390782\n",
      "Epoch36 Train Loss: 0.0877810452580452 Test Loss: 0.17671780834595363\n",
      "Epoch37 Train Loss: 0.0833167796432972 Test Loss: 0.16081041246652603\n",
      "Epoch38 Train Loss: 0.07934989738464356 Test Loss: 0.16170983066161473\n",
      "Epoch39 Train Loss: 0.07944290563464165 Test Loss: 0.1891647363702456\n",
      "Epoch40 Train Loss: 0.06613571238517761 Test Loss: 0.15435322374105453\n",
      "Epoch41 Train Loss: 0.05988616761565208 Test Loss: 0.15746789425611496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[18], line 98\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 98\u001b[0m     matting, color \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     matting \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(matting, dsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_CUBIC)\n\u001b[0;32m    101\u001b[0m     color \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(color, dsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_CUBIC)\n",
      "Cell \u001b[1;32mIn[16], line 39\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(df, idx, normalize_to_float)\u001b[0m\n\u001b[0;32m     37\u001b[0m     clip_img \u001b[38;5;241m=\u001b[39m clip_img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     38\u001b[0m     matting_img \u001b[38;5;241m=\u001b[39m matting_img \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatting_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, clip_img\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = ImageDataset(image_df, n=500)\n",
    "test_dataset = TestImageDataset(image_df, n=60)\n",
    "train_steps=500//4\n",
    "test_steps = 15\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "model = Unet().to('cuda')\n",
    "loss = BCEWithLogitsLoss()\n",
    "opt = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "torch.device('cuda')\n",
    "bestTestLoss = 2\n",
    "\n",
    "lossesOverTime = [[]]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        (images,masks) = (images.to('cuda'), masks.to('cuda'))\n",
    "        y_pred = model(images)\n",
    "        current_loss = loss(y_pred, masks)\n",
    "        opt.zero_grad()\n",
    "        current_loss.backward()\n",
    "        opt.step()\n",
    "        train_loss+=current_loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, masks in test_loader:\n",
    "            (images,masks) = (images.to('cuda'), masks.to('cuda'))\n",
    "            y_pred = model(images)\n",
    "            current_loss = loss(y_pred, masks)\n",
    "            test_loss+=current_loss.item()\n",
    "\n",
    "    train_loss = train_loss / train_steps\n",
    "    test_loss = test_loss / test_steps\n",
    "    lossesOverTime.append([train_loss, test_loss])\n",
    "    if(test_loss < bestTestLoss):\n",
    "        bestTestLoss = test_loss\n",
    "        torch.save(model, \"best_model.pt\")\n",
    "\n",
    "    print(\"Epoch\" + str(epoch) + \" Train Loss: \" + str(train_loss) + \" Test Loss: \" + str(test_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b2fe334680>,\n",
       " <matplotlib.lines.Line2D at 0x2b2fe334b90>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossOverTimneToEnd = lossesOverTime[1:]\n",
    "print()\n",
    "plt.plot(lossOverTimneToEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m color, mask \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m37\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[18], line 71\u001b[0m, in \u001b[0;36mUnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 71\u001b[0m     s1, p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     s2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me2(p1)\n\u001b[0;32m     73\u001b[0m     s3, p3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39me3(p2)\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mencoder_block.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, p\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mconv_block.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\c1a55\\Documents\\TheMattingMen\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "color, mask = train_dataset.__getitem__(37)\n",
    "plt.imshow(model(color))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
